{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "247d3dca-a80b-401f-a7c1-0bf4071af32b",
   "metadata": {},
   "source": [
    "# Project By Varun Gehlot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcfcbd7-076a-411a-81fe-376d252e4a69",
   "metadata": {},
   "source": [
    "## Objective:\n",
    "\n",
    "### To Develop a MLP Neural network Bag-of-Words Model for Movie Reviews Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c649a916-6e2e-4a59-b903-1c3ed4b2131a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\varun\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\varun\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\varun\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 50)                905700    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 905751 (3.46 MB)\n",
      "Trainable params: 905751 (3.46 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\varun\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\varun\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "57/57 [==============================] - 1s 3ms/step - loss: 0.4825 - accuracy: 0.7806\n",
      "Epoch 2/10\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0770 - accuracy: 0.9900\n",
      "Epoch 3/10\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0203 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0090 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0048 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 8.6014e-04 - accuracy: 1.0000\n",
      "Accuracy: 94 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n# adding all docs to vocab\\nproccess_doc('txt_sentoken/pos',vocab)\\nproccess_doc('txt_sentoken/neg',vocab)\\n\\nprint(len(vocab))\\n\\n# keeeping tokens with minimum occurances\\nmin_occurances = 2\\ntokens = [w for w,x in vocab.items() if x > min_occurances]\\nprint(len(tokens))\\n\\n#save_file(tokens,'Vocabulary.txt')\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from os import listdir\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "def load_doc(filename):\n",
    "    with open(filename,'r') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "def clean_doc(doc):\n",
    "    #  tokenization, puncts, stopwowds, only alphas, filter short words > 1\n",
    "    en_stopWords = stopwords.words('english')\n",
    "    tokens = doc.split()\n",
    "    no_punct_sw = [w for w in tokens if w not in punctuation and w not in en_stopWords]\n",
    "    only_alpha = [w for w in no_punct_sw if w.isalpha()]\n",
    "    clean_doc = [w for w in only_alpha if len(w) > 1]\n",
    "    return clean_doc\n",
    "\n",
    "'''\n",
    "def add_doc_to_vocab(filename,vocab):\n",
    "    doc = load_doc(filename)\n",
    "    tokens = clean_doc(doc)\n",
    "    # update() updates the Counter dictionary\n",
    "    vocab.update(tokens)\n",
    "'''   \n",
    "# loading, cleaning, filtering out tokens not in the vocabulary, then returning the document as a string of white space separated tokens \n",
    "def doc_to_line(filename,vocab):\n",
    "    doc = load_doc(filename)\n",
    "    tokens = clean_doc(doc)\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    return ' '.join(tokens)\n",
    "    \n",
    "def proccess_doc(directory,vocab, is_train):\n",
    "    lines = []\n",
    "    for filename in listdir(directory):\n",
    "        # skipping 10% test data that is from cv900 to cv999\n",
    "        if is_train and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_train and not filename.startswith('cv9'):\n",
    "            continue\n",
    "    # creating a full path of file to open\n",
    "        path = directory + '/' + filename\n",
    "    # adding doc to vocabulary\n",
    "        ''' add_doc_to_vocab(path,vocab) '''\n",
    "        line = doc_to_line(path,vocab)\n",
    "        lines.append(line)\n",
    "    return lines\n",
    "\n",
    "# loads documents and labels them 0 and 1\n",
    "def load_clean_dataset(vocab, is_train):\n",
    "    pos = proccess_doc('txt_sentoken/neg', vocab, is_train)\n",
    "    neg = proccess_doc('txt_sentoken/pos', vocab, is_train)\n",
    "    docs = neg + pos\n",
    "    labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
    "    return docs, labels\n",
    "    \n",
    "def save_file(lines,filename):\n",
    "    # saving each token in new line \n",
    "    data = '\\n'.join(lines)\n",
    "    with open(filename,'w') as file:\n",
    "        file.write(data)\n",
    "    \n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "    \n",
    "    \n",
    "def define_model(n_words):\n",
    "    # defining MLP network\n",
    "    model = Sequential() \n",
    "    model.add(Dense(50, input_shape = (n_words,), activation = 'relu'))\n",
    "    model.add(Dense(1,activation= 'sigmoid'))\n",
    "    # compiling network (configuration)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    # summarizing model\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "#vocab = Counter()\n",
    "# loading vocabulary file\n",
    "vocab = load_doc('Vocabulary.txt')\n",
    "vocab = set(vocab.split())\n",
    "\n",
    "'''\n",
    "# loading all training reviews\n",
    "docs, labels = load_clean_dataset(vocab)\n",
    "'''\n",
    "\n",
    "\n",
    "# loading all reviews\n",
    "train_docs, y_train = load_clean_dataset(vocab, True)\n",
    "test_docs, y_test = load_clean_dataset(vocab, False)\n",
    "\n",
    "# convert y_train and y_test from list to arrays\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# creating tokenizer\n",
    "tokenizer = create_tokenizer(train_docs)\n",
    "\n",
    "# encoding data\n",
    "Xtrain = tokenizer.texts_to_matrix(train_docs,mode= 'binary')\n",
    "Xtest = tokenizer.texts_to_matrix(test_docs, mode= 'binary')\n",
    "\n",
    "#print(Xtrain.shape, Xtest.shape)\n",
    "\n",
    "# defining the model\n",
    "n_words = Xtest.shape[1]\n",
    "model = define_model(n_words)\n",
    "\n",
    "# fitting network\n",
    "model.fit(Xtrain, y_train, epochs = 10, verbose = 1)\n",
    "\n",
    "# evaluating\n",
    "loss, acc = model.evaluate(Xtest, y_test, verbose = 0) \n",
    "print(f'Accuracy: {round(acc * 100)} %')\n",
    "\n",
    "'''\n",
    "# summarize what we have\n",
    "print(len(docs),len(labels))\n",
    "'''\n",
    "\n",
    "'''\n",
    "# adding all docs to vocab\n",
    "proccess_doc('txt_sentoken/pos',vocab)\n",
    "proccess_doc('txt_sentoken/neg',vocab)\n",
    "\n",
    "print(len(vocab))\n",
    "\n",
    "# keeeping tokens with minimum occurances\n",
    "min_occurances = 2\n",
    "tokens = [w for w,x in vocab.items() if x > min_occurances]\n",
    "print(len(tokens))\n",
    "\n",
    "#save_file(tokens,'Vocabulary.txt')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1003d29a-cc36-4547-8bcd-a73482cf6a27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
